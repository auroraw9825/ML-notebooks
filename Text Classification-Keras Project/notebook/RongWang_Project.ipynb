{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b28312",
   "metadata": {},
   "source": [
    "Rong Wang   1619779944   DSCI552 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e614a9",
   "metadata": {},
   "source": [
    "<b>1. Text Classification</b>\n",
    "\n",
    "It is highly recommended that you complete this project using Keras1 and Python.\n",
    "\n",
    "1 https://keras.io\n",
    "\n",
    "(a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716de16",
   "metadata": {},
   "source": [
    "<b>(b) Data Exploration and Pre-processing</b>\n",
    "\n",
    "i. You can use binary encoding for the sentiments , i.e y = 1 for positive senti-\n",
    "ments and y = −1 for negative sentiments.\n",
    "\n",
    "ii. The data are pretty clean. Remove the punctuation and numbers from the\n",
    "data.\n",
    "\n",
    "iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb7bc8",
   "metadata": {},
   "source": [
    "iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "641865ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46769\n"
     ]
    }
   ],
   "source": [
    "# Reference: \n",
    "# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "unique_words = set()\n",
    "review_len = []\n",
    "reviews = []\n",
    "\n",
    "mypath_pos = \"../data/pos/\"\n",
    "mypath_neg = \"../data/neg/\"\n",
    "filenames_pos = [join(mypath_pos, f) for f in listdir(mypath_pos) if isfile(join(mypath_pos, f))]\n",
    "filenames_neg = [join(mypath_neg, f) for f in listdir(mypath_neg) if isfile(join(mypath_neg, f))]\n",
    "filenames_pos.sort()\n",
    "filenames_neg.sort()\n",
    "\n",
    "for filename in filenames_pos+filenames_neg:\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.read()\n",
    "    # print(line)\n",
    "    \n",
    "#     remove_punc = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    remove_punc = re.sub(r'[^\\w\\s]', '', line)  # remove punctuations\n",
    "    # print(remove_punc)\n",
    "    remove_num = re.sub(\"\\d+\", \" \", remove_punc)  # remove numbers\n",
    "#     print(remove_num)\n",
    "    remove_underline = remove_num.replace('_', '')  # remove _\n",
    "    string_lower = remove_underline.lower()\n",
    "    \n",
    "    reviews.append(string_lower)\n",
    "    \n",
    "    word_list = string_lower.split()  # argument is omitted, split by whitespace, newlines \\n, and tabs \\t. Consecutive whitespace is processed together.\n",
    "#     print(word_list)\n",
    "    review_len.append(len(word_list))\n",
    "    unique_words.update(word_list)\n",
    "print(len(unique_words))\n",
    "# print(sorted(unique_words))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4bcb6",
   "metadata": {},
   "source": [
    "v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "30bf2fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg review length:  644.407\n",
      "std review length:  284.99187769303177\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.geeksforgeeks.org/python-standard-deviation-of-list/\n",
    "\n",
    "import statistics\n",
    "\n",
    "print(\"avg review length: \", statistics.mean(review_len))\n",
    "print(\"std review length: \", statistics.pstdev(review_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676546a",
   "metadata": {},
   "source": [
    "vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "acdf0d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference: https://www.tutorialspoint.com/how-to-plot-a-histogram-using-matplotlib-in-python-with-a-list-of-data\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(review_len, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a88fd",
   "metadata": {},
   "source": [
    "vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to rep- resent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method.2\n",
    "\n",
    "2Keras has an API called Tokenizer. It can yield bag of words, one-hot encoded features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "897ad2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl (217.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.5 MB 354 kB/s eta 0:00:011    |████████                        | 54.4 MB 444 kB/s eta 0:06:08\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.8.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 24.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.2.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001b[K     |████████████████████████████████| 962 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 6.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.0-cp39-cp39-macosx_10_10_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 35.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aurorawong/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=de083d2304203571d87ecde7ffab5b18ebd8bff35c1bd09f354648c3d93317e8\n",
      "  Stored in directory: /Users/aurorawong/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4c436359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# file Tokenize.webarchive from professor\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the reviews\n",
    "t.fit_on_texts(reviews)\n",
    "# summarize what was learned\n",
    "# print(t.word_counts)\n",
    "# print(t.document_count)\n",
    "word_index = t.word_index # rank of each work, dict\n",
    "# print(t.word_docs)\n",
    "# integer encode documents\n",
    "# encoded_docs = t.texts_to_matrix(reviews, mode='count')\n",
    "\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545f416",
   "metadata": {},
   "source": [
    "viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f8fc4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737\n"
     ]
    }
   ],
   "source": [
    "# 70% length\n",
    "sorted_review_len = sorted(review_len)\n",
    "print(sorted(review_len)[1400])\n",
    "# if > 737, truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd756a9",
   "metadata": {},
   "source": [
    "ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L.3\n",
    "\n",
    "3Keras has pad sequences for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "80a6e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "\n",
    "train_x = reviews[:700] + reviews[1000:1700]\n",
    "test_x = reviews[700:1000] + reviews[1700:2000]\n",
    "train_y = array([1] * 700 + [0] * 700)\n",
    "test_y = array([1] * 300 + [0] * 300)\n",
    "\n",
    "review_rank_train = []\n",
    "for line in train_x:\n",
    "#     print(line)\n",
    "    line_words = line.split()\n",
    "#     print(line_words)\n",
    "    if len(line_words) > 737:\n",
    "        new_line = line_words[:737]\n",
    "        word_rank = map(lambda x: word_index[x], new_line)\n",
    "    else:\n",
    "        word_rank = map(lambda x: word_index[x], line_words)\n",
    "    rank_list = list(word_rank)\n",
    "#     print(rank_list)\n",
    "    review_rank_train.append(rank_list)\n",
    "# print(review_rank[0])\n",
    "\n",
    "review_seq_train = pad_sequences(review_rank_train, padding='post')\n",
    "# print(review_seq[3])\n",
    "\n",
    "\n",
    "review_rank_test = []\n",
    "for line in test_x:\n",
    "#     print(line)\n",
    "    line_words = line.split()\n",
    "#     print(line_words)\n",
    "    if len(line_words) > 737:\n",
    "        new_line = line_words[:737]\n",
    "        word_rank = map(lambda x: word_index[x], new_line)\n",
    "    else:\n",
    "        word_rank = map(lambda x: word_index[x], line_words)\n",
    "    rank_list = list(word_rank)\n",
    "#     print(rank_list)\n",
    "    review_rank_test.append(rank_list)\n",
    "# print(review_rank[0])\n",
    "\n",
    "review_seq_test = pad_sequences(review_rank_test, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d976346",
   "metadata": {},
   "source": [
    "<b>(c) Word Embeddings</b>\n",
    "\n",
    "i. One can use tokenized text as inputs to a deep neural network. However, a re- cent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings. “Word embedding is a term used for representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.”4. Most deep learning modules (including Keras) provide a convenient way to convert positive integer rep- resentations of words into a word embedding by an “Embedding layer.” The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the “output dimension.” We would like to use a word embedding layer for this project. Assume that we are inter- ested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document.5 If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 × L matrix.\n",
    "\n",
    "4 https://en.wikipedia.org/wiki/Word_embedding\n",
    "\n",
    "5 This is done by...\n",
    "Example: model.add(Embedding(top words, 32, input length=max words)), where max words=L.\n",
    "\n",
    "ii. Flatten the matrix of each document to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "004f263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_18 (Embedding)    (None, 737, 32)           1496640   \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 23584)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,496,640\n",
      "Trainable params: 1,496,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Reference: \n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# https://keras.io/api/layers/core_layers/embedding/#embedding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "top_words = len(unique_words)+1  # 5000+1, assume only interested in the top 5000 words.\n",
    "max_words = 737 # len(unique_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ffc31",
   "metadata": {},
   "source": [
    "<b>(d) Multi-Layer Perceptron</b>\n",
    "\n",
    "i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c89790e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_18 (Embedding)    (None, 737, 32)           1496640   \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 23584)             0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 50)                1179250   \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,681,041\n",
      "Trainable params: 2,681,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 13ms/step - loss: 0.6986 - accuracy: 0.5271\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda9911ea30>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://keras.io/api/layers/activations/\n",
    "# https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "# https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
    "\n",
    "# print(review_seq_train[3])\n",
    "# print(review_seq_train[4])\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model.add(Dense(50, activation='relu'))  # activation=activations.relu\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')) # output\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(review_seq_train, train_y, epochs=2, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1c6ee",
   "metadata": {},
   "source": [
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8a3b01e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data:\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.9107\n",
      "Accuracy: 0.910714\n",
      "\n",
      "For test data:\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.6433\n",
      "Accuracy: 0.643333\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(\"For train data:\")\n",
    "loss_train, accuracy_train = model.evaluate(review_seq_train, train_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_train))\n",
    "print(\"\\nFor test data:\")\n",
    "loss_test, accuracy_test = model.evaluate(review_seq_test, test_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f53718",
   "metadata": {},
   "source": [
    "<b>(e) One-Dimensional Convolutional Neural Network:</b>\n",
    "\n",
    "Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "\n",
    "i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1fcb5222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, 737, 32)           1496640   \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 735, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 367, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 11744)             0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 50)                587250    \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,092,145\n",
      "Trainable params: 2,092,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "140/140 [==============================] - 4s 17ms/step - loss: 0.7021 - accuracy: 0.5014\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 2s 18ms/step - loss: 0.6572 - accuracy: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda3c8e9eb0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: \n",
    "# https://stackoverflow.com/questions/50005092/how-to-build-1d-convolutional-neural-network-in-keras-python\n",
    "# https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/\n",
    "# https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "# https://keras.io/api/layers/pooling_layers/max_pooling1d/\n",
    "# https://stackoverflow.com/questions/57430717/input-0-of-layer-conv1d-1-is-incompatible-with-the-layer-expected-ndim-3-found\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "model_e = Sequential()\n",
    "model_e.add(Embedding(top_words, 32, input_length=max_words))\n",
    "\n",
    "model_e.add(Conv1D(filters=32, kernel_size=3))  # , input_shape=(None, 23584)\n",
    "model_e.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "# first Conv1D, then flatten, because Conv1D need a 3d input, but later need flatten to make it 2d.\n",
    "model_e.add(Flatten())\n",
    "model_e.add(Dense(50, activation='relu'))  # activation=activations.relu\n",
    "model_e.add(Dropout(0.2))\n",
    "model_e.add(Dense(50, activation='relu'))\n",
    "model_e.add(Dropout(0.5))\n",
    "model_e.add(Dense(50, activation='relu'))\n",
    "model_e.add(Dropout(0.5))\n",
    "\n",
    "model_e.add(Dense(1, activation='sigmoid')) # output\n",
    "\n",
    "# compile the model\n",
    "model_e.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model_e.summary())\n",
    "\n",
    "# fit the model\n",
    "model_e.fit(review_seq_train, train_y, epochs=2, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec6fbb",
   "metadata": {},
   "source": [
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1da401a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data:\n",
      "44/44 [==============================] - 0s 4ms/step - loss: 0.4112 - accuracy: 0.8621\n",
      "Accuracy: 0.862143\n",
      "\n",
      "For test data:\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.5983\n",
      "Accuracy: 0.598333\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(\"For train data:\")\n",
    "loss_train, accuracy_train = model_e.evaluate(review_seq_train, train_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_train))\n",
    "print(\"\\nFor test data:\")\n",
    "loss_test, accuracy_test = model_e.evaluate(review_seq_test, test_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe7e9a",
   "metadata": {},
   "source": [
    "<b>(f) Long Short-Term Memory Recurrent Neural Network:</b>\n",
    "\n",
    "The structure of the LSTM we are going to use is shown in the following figure.\n",
    "\n",
    "i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bd001fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    (None, 737, 32)           1496640   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513,665\n",
      "Trainable params: 1,513,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/13\n",
      "140/140 [==============================] - 16s 105ms/step - loss: 0.6932 - accuracy: 0.5079\n",
      "Epoch 2/13\n",
      "140/140 [==============================] - 14s 103ms/step - loss: 0.6706 - accuracy: 0.5729\n",
      "Epoch 3/13\n",
      "140/140 [==============================] - 14s 102ms/step - loss: 0.5952 - accuracy: 0.6557\n",
      "Epoch 4/13\n",
      "140/140 [==============================] - 14s 103ms/step - loss: 0.5163 - accuracy: 0.6729\n",
      "Epoch 5/13\n",
      "140/140 [==============================] - 15s 106ms/step - loss: 0.4928 - accuracy: 0.6721\n",
      "Epoch 6/13\n",
      "140/140 [==============================] - 15s 105ms/step - loss: 0.4785 - accuracy: 0.6771\n",
      "Epoch 7/13\n",
      "140/140 [==============================] - 15s 104ms/step - loss: 0.4769 - accuracy: 0.6814\n",
      "Epoch 8/13\n",
      "140/140 [==============================] - 14s 103ms/step - loss: 0.4699 - accuracy: 0.6814\n",
      "Epoch 9/13\n",
      "140/140 [==============================] - 17s 121ms/step - loss: 0.4789 - accuracy: 0.6807\n",
      "Epoch 10/13\n",
      "140/140 [==============================] - 16s 112ms/step - loss: 0.4731 - accuracy: 0.6857\n",
      "Epoch 11/13\n",
      "140/140 [==============================] - 15s 107ms/step - loss: 0.4787 - accuracy: 0.6821\n",
      "Epoch 12/13\n",
      "140/140 [==============================] - 16s 113ms/step - loss: 0.4715 - accuracy: 0.6864\n",
      "Epoch 13/13\n",
      "140/140 [==============================] - 15s 104ms/step - loss: 0.4696 - accuracy: 0.6836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda4a52ac70>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: \n",
    "# https://keras.io/guides/working_with_rnns/\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model_f = Sequential()\n",
    "model_f.add(Embedding(top_words, 32, input_length=max_words))\n",
    "# model_f.add(Flatten())\n",
    "\n",
    "model_f.add(LSTM(32))\n",
    "model_f.add(Dropout(0.2))\n",
    "\n",
    "model_f.add(Dense(256, activation='relu'))  # activation=activations.relu\n",
    "model_f.add(Dropout(0.2))\n",
    "\n",
    "model_f.add(Dense(1, activation='sigmoid')) # output\n",
    "\n",
    "# compile the model\n",
    "model_f.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model_f.summary())\n",
    "\n",
    "# fit the model\n",
    "model_f.fit(review_seq_train, train_y, epochs=13, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d381f",
   "metadata": {},
   "source": [
    "I choose epoch=13, because if more than this, seems like the model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cd714",
   "metadata": {},
   "source": [
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4285e58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data:\n",
      "44/44 [==============================] - 2s 33ms/step - loss: 0.4661 - accuracy: 0.6857\n",
      "Accuracy: 0.685714\n",
      "\n",
      "For test data:\n",
      "19/19 [==============================] - 1s 34ms/step - loss: 1.1317 - accuracy: 0.5567\n",
      "Accuracy: 0.556667\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(\"For train data:\")\n",
    "loss_train, accuracy_train = model_f.evaluate(review_seq_train, train_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_train))\n",
    "print(\"\\nFor test data:\")\n",
    "loss_test, accuracy_test = model_f.evaluate(review_seq_test, test_y, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8817c",
   "metadata": {},
   "source": [
    "The accuracy is not really high, but it can be increased to about 80% afte removing stop words, because stop words are taking a high ranking in the word_index. Code is not attached as not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a64bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
